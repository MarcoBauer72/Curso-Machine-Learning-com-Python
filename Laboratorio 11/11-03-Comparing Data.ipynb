{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Comparing Data\nYou'll often want to compare data in your dataset, to see if you can discern trends or relationships.\n\n## Univariate Data\n*Univariate* data is data that consist of only one variable or feature. While it may initially seem as though there's not much we can do to analyze univariate data, we've already seen that we can explore its distribution in terms of measures of central tendency and measures of variance. We've also seen how we can visualize this distribution using histograms and box plots.\n\nHere's a reminder of how you can visualize the distribution of univariate data, using our student grade data with a few additional observations in the sample:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\nplt.figure()\ndf['Grade'].plot( kind='box', title='Grade Distribution')\nplt.figure()\ndf['Grade'].hist(bins=9)\nplt.show()\nprint(df.describe())\nprint('median: ' + str(df['Grade'].median()))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": "           Grade\ncount  15.000000\nmean   51.533333\nstd    24.642781\nmin     5.000000\n25%    41.000000\n50%    50.000000\n75%    66.000000\nmax    95.000000\nmedian: 50.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Bivariate and Multivariate Data\nIt can often be useful to compare *bivariate* data; in other words, compare two variables, or even more (in which case we call it *multivariate* data).\n\nFor example, our student data includes three numeric variables for each student: their salary, the number of hours they work per week, and their final school grade. Run the following code to see an enlarged sample of this data as a table:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\ndf[['Name', 'Salary', 'Hours', 'Grade']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's suppose you want to compare the distributions of these variables. You might simply create a boxplot for each variable, like this:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\n\ndf.plot(kind='box', title='Distribution', figsize = (10,8))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Hmm, that's not particularly useful is it?\n\nThe problem is that the data are all measured in different scales. Salaries are typically in tens of thousands, while hours and grades are in single or double digits.\n\n### Normalizing Data\nWhen you need to compare data in different units of measurement, you can *normalize* or *scale* the data so that the values are measured in the same proportional scale. For example, in Python you can use a MinMax scaler to normalize multiple numeric variables to a proportional value between 0 and 1 based on their minimum and maximum values. Run the following cell to do this:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\n# Normalize the data\nscaler = MinMaxScaler()\ndf[['Salary', 'Hours', 'Grade']] = scaler.fit_transform(df[['Salary', 'Hours', 'Grade']])\n\n# Plot the normalized data\ndf.plot(kind='box', title='Distribution', figsize = (10,8))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now the numbers on the y axis aren't particularly meaningful, but they're on a similar scale."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Comparing Bivariate Data with a Scatter Plot\nWhen you need to compare two numeric values, a scatter plot can be a great way to see if there is any apparent relationship between them so that changes in the value of one variable affect the value of the other.\n\nLet's look at a scatter plot of *Salary* and *Grade*:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\n# Create a scatter plot of Salary vs Grade\ndf.plot(kind='scatter', title='Grade vs Hours', x='Grade', y='Salary')\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Look closely at the scatter plot. Can you see a diagonal trend in the plotted points, rising up to the right? It looks as though the higher the student's grade is, the higher their salary is.\n\nYou can see the trend more clearly by adding a *line of best fit* (sometimes called a *trendline*) to the plot:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\n# Create a scatter plot of Salary vs Grade\ndf.plot(kind='scatter', title='Grade vs Salary', x='Grade', y='Salary')\n\n# Add a line of best fit\nplt.plot(np.unique(df['Grade']), np.poly1d(np.polyfit(df['Grade'], df['Salary'], 1))(np.unique(df['Grade'])))\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The line of best fit makes it clearer that there is some apparent *colinearity* between these variables (the relationship is *colinear* if one variable's value increases or decreases in line with the other).\n\n### Correlation\nThe apparently colinear relationship you saw in the scatter plot can be verified by calculating a statistic that quantifies the relationship between the two variables. The statistic usually used to do this is *correlation*, though there is also a statistic named *covariance* that is sometimes used. Correlation is generally preferred because the value it produces is more easily interpreted.\n\nA correlation value is always a number between ***-1*** and ***1***.\n- A positive value indicates a positive correlation (as the value of variable *x* increases, so does the value of variable *y*).\n- A negative value indicates a negative correlation (as the value of variable *x* increases, the value of variable *y* decreases).\n- The closer to zero the correlation value is, the weaker the correlation between *x* and *y*.\n- A correlation of exactly zero means there is no apparent relationship between the variables.\n\nThe formula to calculate correlation is:\n\n\\begin{equation}r_{x,y} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i} -\\bar{x})(y_{i} -\\bar{y})}{\\sqrt{\\displaystyle\\sum_{i=1}^{n} (x_{i} -\\bar{x})^{2}(y_{i} -\\bar{y})^{2}}}\\end{equation}\n\n**r<sub>x, y</sub>** is the notation for the *correlation between x and y*.\n\nThe formula is pretty complex, but fortunately Python makes it very easy to calculate the correlation by using the ***corr*** function:"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n                   'Hours':[41,40,36,17,35,39,40],\n                   'Grade':[50,50,46,95,50,5,57]})\n\n# Calculate the correlation between *Salary* and *Grade*\nprint(df['Grade'].corr(df['Salary']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this case, the correlation is just over 0.8; making it a reasonably high positive correlation that indicates salary increases in line with grade.\n\nLet's see if we can find a correlation between *Grade* and *Hours*:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\nr = df['Grade'].corr(df['Hours'])\nprint('Correlation: ' + str(r))\n\n# Create a scatter plot of Salary vs Grade\ndf.plot(kind='scatter', title='Grade vs Hours', x='Grade', y='Hours')\n\n# Add a line of best fit-\nplt.plot(np.unique(df['Grade']), np.poly1d(np.polyfit(df['Grade'], df['Hours'], 1))(np.unique(df['Grade'])))\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this case, the correlation value is just under -0.8; meaning a fairly strong negative correlation in which the number of hours worked decreases as the grade increases. The line of best fit on the scatter plot corroborates this statistic.\n\nIt's important to remember that *correlation* **is not** *causation*. In other words, even though there's an apparent relationship, you can't say for sure that one variable is the cause of the other. In this example, we can say that students who achieved higher grades tend to work shorter hours; but we ***can't*** say that those who work shorter hours do so *because* they achieved a high grade!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Least Squares Regression\nIn the previous examples, we drew a line on a scatter plot to show the *best fit* of the data. In many cases, your initial attempts to identify any colinearity might involve adding this kind of line by hand (or just mentally visualizing it); but as you may suspect from the use of the *numpy.**polyfit*** function in the code above, there are ways to calculate the coordinates for this line mathematically. One of the most commonly used techniques is *least squares regression*, and that's what we'll look at now.\n\nCast your mind back to when you were learning how to solve linear equations, and recall that the *slope-intercept* form of a linear equation lookes like this:\n\n\\begin{equation}y = mx + b\\end{equation}\n\nIn this equation, *y* and *x* are the coordinate variables, *m* is the slope of the line, and *b* is the y-intercept of the line.\n\nIn the case of our scatter plot for our former-student's working hours, we already have our values for *x* (*Grade*) and *y* (*Hours*), so we just need to calculate the intercept and slope of the straight line that lies closest to those points. Then we can form a linear equation that calculates the a new *y* value on that line for each of our *x* (*Grade*) values - to avoid confusion, we'll call this new *y* value *f(x)* (because it's the output from a linear equation function based on *x*). The difference between the original *y* (*Hours*) value and the *f(x)* value is the *error* between our regression line of best fit and the actual *Hours* worked by the former student. Our goal is to calculate the slope and intercept for a line with the lowest overall error.\n\nSpecifically, we define the overall error by taking the error for each point, squaring it, and adding all the squared errors together. The line of best fit is the line that gives us the lowest value for the sum of the squared errors - hence the name *least squares regression*.\n\nSo how do we accomplish this? First we need to calculate the slope (*m*), which we do using this formula (in which *n* is the number of observations in our data sample):\n\n\\begin{equation}m = \\frac{n(\\sum{xy}) - (\\sum{x})(\\sum{y})}{n(\\sum{x^{2}})-(\\sum{x})^{2}}\\end{equation}\n\nAfter we've calculated the slope (*m*), we can use is to calculate the intercept (*b*) like this:\n\n\\begin{equation}b = \\frac{\\sum{y} - m(\\sum{x})}{n}\\end{equation}\n\nLet's look at a simple example that compares the number of hours of nightly study each student undertook with the final grade the student achieved:\n\n| Name     | Study | Grade |\n|----------|-------|-------|\n| Dan      | 1     | 50    |\n| Joann    | 0.75  | 50    |\n| Pedro    | 0.6   | 46    |\n| Rosie    | 2     | 95    |\n| Ethan    | 1     | 50    |\n| Vicky    | 0.2   | 5     |\n| Frederic | 1.2   | 57    |\n\nFirst, let's take each *x* (Study) and *y* (Grade) pair and calculate *x<sup>2</sup>* and *xy*, because we're going to need these to work out the slope:\n\n| Name     | Study | Grade | x<sup>2</sup> | xy   |\n|----------|-------|-------|------|------|\n| Dan      | 1     | 50    | 1    | 50   |\n| Joann    | 0.75  | 50    | 0.55 | 37.5 |\n| Pedro    | 0.6   | 46    | 0.36 | 27.6 |\n| Rosie    | 2     | 95    | 4    | 190  |\n| Ethan    | 1     | 50    | 1    | 50   |\n| Vicky    | 0.2   | 5     | 0.04 | 1    |\n| Frederic | 1.2   | 57    | 1.44 | 68.4 |\n\nNow we'll sum *x*, *y*, *x<sup>2</sup>*, and *xy*:\n\n| Name     | Study | Grade | x<sup>2</sup> | xy   |\n|----------|-------|-------|------|------|\n| Dan      | 1     | 50    | 1    | 50   |\n| Joann    | 0.75  | 50    | 0.55 | 37.5 |\n| Pedro    | 0.6   | 46    | 0.36 | 27.6 |\n| Rosie    | 2     | 95    | 4    | 190  |\n| Ethan    | 1     | 50    | 1    | 50   |\n| Vicky    | 0.2   | 5     | 0.04 | 1    |\n| Frederic | 1.2   | 57    | 1.44 | 68.4 |\n| **&Sigma;**      | **6.75**  | **353**   | **8.4025**| **424.5**  |\n\nOK, now we're ready to calculate the slope for our *7* observations:\n\n\\begin{equation}m = \\frac{(7\\times 424.5) - (6.75\\times353)}{(7\\times8.4025)-6.75^{2}}\\end{equation}\n\nWhich is:\n\n\\begin{equation}m = \\frac{2971.5 - 2382.75}{58.8175-45.5625}\\end{equation}\n\nSo:\n\n\\begin{equation}m = \\frac{588.75}{13.255} \\approx 44.4172\\end{equation}\n\nNow we can calculate *b*:\n\n\\begin{equation}b = \\frac{353 - (44.4172\\times6.75)}{7}\\end{equation}\n\nWhich simplifies to:\n\n\\begin{equation}b = \\frac{53.18389}{7} = 7.597699\\end{equation}\n\nNow we have our linear function:\n\n\\begin{equation}f(x) = 44.4172x + 7.597699\\end{equation}\n\nWe can use this for each *x* (Study) value to calculate the *y* values for the regression line (*f(x)*), and we can subtract the original *y* (Grade) from these to calculate the error for each point:\n\n| Name     | Study | Grade | *f(x)* | Error |\n|----------|-------|-------|------|------ |\n| Dan      | 1     | 50    |52.0149 |2.0149 |\n| Joann    | 0.75  | 50    |40.9106 |-9.0894|\n| Pedro    | 0.6   | 46    |34.2480 |-11.752|\n| Rosie    | 2     | 95    |96.4321 |1.4321 |\n| Ethan    | 1     | 50    |52.0149 |2.0149 |\n| Vicky    | 0.2   | 5     |16.4811 |11.4811|\n| Frederic | 1.2   | 57    |60.8983 |3.8983 |\n\nAs you can see, the *f(x)* values are mostly quite close to the actual *Grade* values, and the errors (which when we're comparing estimated values from a function with actual known values we we often call *residuals*) are generally pretty small.\n\nLet's plot the least squares regression line with the actual values:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n                   'Study':[1,0.75,0.6,2,1,0.2,1.2],\n                   'Grade':[50,50,46,95,50,5,57],\n                   'fx':[52.0159,40.9106,34.2480,96.4321,52.0149,16.4811,60.8983]})\n\n# Create a scatter plot of Study vs Grade\ndf.plot(kind='scatter', title='Study Time vs Grade Regression', x='Study', y='Grade', color='red')\n\n# Plot the regression line\nplt.plot(df['Study'],df['fx'])\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this case, the line fits the middle values fairly well, but is less accurate for the outlier at the low end. This is often the case, which is why statisticians and data scientists often *treat* outliers by removing them or applying a threshold value; though in this example there are too few data points to conclude that the data points are really outliers.\n\nLet's look at a slightly larger dataset and apply the same approach to compare *Grade* and *Salary*:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\n# Calculate least squares regression line\ndf['x2'] = df['Grade']**2\ndf['xy'] = df['Grade'] * df['Salary']\nx = df['Grade'].sum()\ny = df['Salary'].sum()\nx2 = df['x2'].sum()\nxy = df['xy'].sum()\nn = df['Grade'].count()\nm = ((n*xy) - (x*y))/((n*x2)-(x**2))\nb = (y - (m*x))/n\ndf['fx'] = (m*df['Grade']) + b\ndf['error'] = df['fx'] - df['Salary']\n\nprint('slope: ' + str(m))\nprint('y-intercept: ' + str(b))\n\n# Create a scatter plot of Grade vs Salary\ndf.plot(kind='scatter', title='Grade vs Salary Regression', x='Grade', y='Salary', color='red')\n\n# Plot the regression line\nplt.plot(df['Grade'],df['fx'])\n\nplt.show()\n\n# Show the original x,y values, the f(x) value, and the error\ndf[['Grade', 'Salary', 'fx', 'error']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this case, we used Python expressions to calculate the *slope* and *y-intercept* using the same approach and formula as before. In practice, Python provides great support for statistical operations like this; and you can use the ***linregress*** function in the *scipy.stats* package to retrieve the *slope* and *y-intercept* (as well as the *correlation*, *p-value*, and *standard error*) for a matched array of *x* and *y* values (we'll discuss *p-values* later!).\n\nHere's the Python code to calculate the regression line variables using the ***linregress*** function:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\n# Get the regression line slope and intercept\nm, b, r, p, se = stats.linregress(df['Grade'], df['Salary'])\n\ndf['fx'] = (m*df['Grade']) + b\ndf['error'] = df['fx'] - df['Salary']\n\nprint('slope: ' + str(m))\nprint('y-intercept: ' + str(b))\n\n# Create a scatter plot of Grade vs Salary\ndf.plot(kind='scatter', title='Grade vs Salary Regression', x='Grade', y='Salary', color='red')\n\n# Plot the regression line\nplt.plot(df['Grade'],df['fx'])\n\nplt.show()\n\n# Show the original x,y values, the f(x) value, and the error\ndf[['Grade', 'Salary', 'fx', 'error']]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that the *slope* and *y-intercept* values are the same as when we worked them out using the formula.\n\nSimilarly to the simple study hours example, the regression line doesn't fit the outliers very well. In this case, the extremes include a student who scored only 5, and a student who scored 95. Let's see what happens if we remove these students from our sample:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\n\ndf = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n\ndf = df[(df['Grade'] > 5) & (df['Grade'] < 95)]\n\n# Get the regression line slope and intercept\nm, b, r, p, se = stats.linregress(df['Grade'], df['Salary'])\n\ndf['fx'] = (m*df['Grade']) + b\ndf['error'] = df['fx'] - df['Salary']\n\nprint('slope: ' + str(m))\nprint('y-intercept: ' + str(b))\n\n# Create a scatter plot of Grade vs Salary\ndf.plot(kind='scatter', title='Grade vs Salary Regression', x='Grade', y='Salary', color='red')\n\n# Plot the regression line\nplt.plot(df['Grade'],df['fx'])\n\nplt.show()\n\n# Show the original x,y values, the f(x) value, and the error\ndf[['Grade', 'Salary', 'fx', 'error']]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With the outliers removed, the line is a slightly better overall fit to the data.\n\nOne of the neat things about regression is that it gives us a formula and some constant values that we can use to estimate a *y* value for any *x* value. We just need to apply the linear function using the *slope* and *y-intercept* values we've calculated from our sample data. For example, suppose a student named Fabio graduates from our school with a final grade of **62**. We can use our linear function with the *slope* and *y-intercept* values we calculated with Python to estimate what salary he can expect to earn:\n\n\\begin{equation}f(x) = (1424.50\\times62) - 7822.24 \\approx 80,497 \\end{equation}\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}