{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 3, Lab 3 - Causal Claims\n===============================\n\nIn this lab, we briefly explore the causal claim. Recall that if you\nwant to truly understand your variables, you want to draw cause-effect\nconclusions. \n\nIn this lab, we will use the `effsize` package for measuring effect size\nand the `ggplot2` package for data visualization. We will also briefly\nexplore the `ggridges` package."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#### LOAD PACKAGES ####\nfrom scipy import stats\nimport scipy.stats as ss\nimport pandas as pd\nimport numpy as np\nimport statsmodels.stats.weightstats as ws\nfrom statsmodels.stats.power import tt_ind_solve_power\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Weaknesses of Association Claims for Deep Understanding\n========================================================\n\nAssociation claims are useful; they let you know what variables correlate\nwith each other. However, they don't tell you what will happen if you\nintervene and act in the world in new ways. For example, imagine you have\nbeen analyzing data at your organization and find that employees who are\nless stressed tend to be more productive. It may be very unclear *why* that\nrelationship exists. Some possibilities include:\n\n1.  Stress reduces productivity\n2.  Productivity reduces stress\n3.  Some other variable is causing both of them\n\nIt is worth putting real thought into all of these.\n\n1.  For example, it is reasonable that stress reduces productivity.\n2.  However, it is also reasonable that getting things done may take\n    stress off of the shoulders of employees as they clear projects off\n    their to-do lists.\n3.  It is also possible that something else may be casing both high\n    stress and low productivity, such as obligations outside of work,\n    health issues, etc.\n\nAll three of these have different implications for how to increase\nproductivity:\n\n1.  The first possibility suggests that reducing stress might actually\n    help.\n2.  The second possibility suggests that reducing stress will not help\n    (but finding other contributors to productivity might, so we should\n    go looking for those). Time for more research!\n3.  The third possibility suggests that attempts to reduce stress would\n    do nothing to increase productivity because the real problem is the\n    unmeasured *prior cause* of the stress. For example, if health\n    issues are causing people to be more stressed and less productive,\n    then the desired boost to productivity will not come from reducing\n    stress but from fixing the underlying health issues.\n\nIn conclusion, association claims are limited in their ability to help\nyou draw cause-effect conclusions. Or, to put it differently, we could\n*predict* the productivity of an employee given their stress levels, but\nwe wouldn't know how to actually *improve* productivity given this\ninformation. Association claims simply don't really tell you what is\ncausing what.\n\nSolution: The Experiment\n========================\n\nTo solve this problem, we run an experiment. Imagine we randomly assign\n250 employees to either a stress-reduction treatment or a \"business-as-usual\"\ncontrol group. After 7 weeks, the productivity of these employees is\nassessed.\n\nBecause employees are *randomly* assigned to groups *by the researcher*,\nthe research can be confident that\n\n-   The two groups were approximately equal to begin with (this can be\n    checked, if desired)\n-   Any differences at the end of the study are due to the treatment\n\nWe can draw this conclusion because we will be very careful to treat the\ngroups *identically* in every way, except for the treatment. We must be\n**exceedingly** careful on this point, as any unintended differences in\ntreatment (the messaging we give them, the scheduling, workload, etc.)\ncould accidentally cause a second systematic difference between our\ngroups, and then we would not be sure what was really responsible for\nany effects we end up seeing. This is known as a *confound* and it would\nrun our experiment. We will be very sure not to allow this to happen,\nusing strict protocols, scripts, email templates, etc. We would be very\ncareful to manage expectations so neither group had different\nexpectations (possibly keeping our employees blind to some of the\ndetails, or keeping managers in the dark). Our goal will be to keep\n**everything the same between our groups**, tangibly and mentally,\nexcept for the actual treatment itself. This will take very detailed and\nrigorous planning, but it is worth it. A small-scale pilot of an\nintervention program will take some rigorous planning, but it is much\nless expensive than rolling out a company-wide stress program only to\nfind it is a waste of money and ineffective (as might happen in many\norganizations).\n\nAnalyzing the Experiment\n========================\n\nTo analyze the experiment, a simple independent-groups *t*-test can be\nperformed. This compares the means of the two groups to determine if\nthey are statistically significantly different.\n\nImagine the study is done; the data are called \"causal.csv\" and are in\nthe github folder for this lab."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#### LOAD DATA ####\nimport pandas as pd\ndat = pd.read_csv(\"datasets/causal.csv\")\n\n# Inspect data\nprint(dat.columns)\n\nprint('\\n')\nprint(dat.head())\n\nprint('\\n')\nprint(dat.group.unique())\n\ndat.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see we have three variables, an unnamed id variable, a variable\nlisting the group, and the productivity scores of the employees on a 1-7\nscale. We want to compare the two groups, and we can do so by looking at\nthe means.\n\nWe can quickly request more detailed statistics. The values of `prod` can be grouped by the `group` using the Pandas `groupby` method. The mean of each group is then computed. The same recipe can be used to compute the standard deviation of the groups. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat[['group','prod']].groupby('group').mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat[['group','prod']].groupby('group').std()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that intervention group has a slightly higher average\nproductivity score. We can next test the null hypothesis to see if this\ndifference is significant.\n\nRecall that the null hypothesis always says that the **effect is absent\nin the population** and that the sample result is an artifact of random\nchance. In symbols, this means that the difference between the group\naverages is exactly zero in the population.\n\n*H*<sub>0</sub> : *μ*<sub>*g**r**o**u**p*1</sub> − *μ*<sub>*g**r**o**u**p*2</sub> = 0\n Remember that *μ* refers to the population average, so this is saying\nthat the population difference is exactly zero. Any difference observed\nin our sample is therefore due to random chance. We run our *t*-test to\nconsider this possibility.\n\nRecall that a *t*-test compares the size of the *observed difference*\n($\\bar{x}_{1} - \\bar{x}_{2}$) against the value in the null hypothesis\n(zero), divided by what is typically expected by chance:\n\n$$t= \\frac{result - null }{chance}$$\n For the two-group *t*-test, the \"result\" is the difference between the\ngroup averages in the sample, the \"null\" states the difference, and\n\"chance\" is the standard error of that difference.\n\nHow can we run our test? The default in R is to run the \"Welch\" version\nof the test. This version of the test does *not* make any assumptions\nabout the variances of the two groups.\n\n$$t'= \\frac{result - null }{chance}= \\frac{(\\bar{x}_{1}-\\bar{x}_{2}) - 0 }{\\sqrt{\\frac{\\hat{\\sigma}_1^2}{n_{1}}+\\frac{\\hat{\\sigma}_2^2}{n_{2}}}}$$\n The bottom looks complicated but is simply a measure of the standard\nerror of the size of the difference between our two groups. We can\nexplore the details of this equation in a later lab. For now, let's run\nthe test and interpret the result.\n\nThe Python code in the function below does the following:\n1. Compute the difference of means. \n2. The `ttest_ind` function from the scipy.stats package to compute the t statistic and p-value.\n3. The `tconfint_diff` function is used to compute the confidence interval.\n4. The `dof_satt` function estimates the degrees of freedom. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def t_test_two_samp(df, alpha, alternative='two-sided'):\n    \n    a = df[df.group == 'control']['prod']\n    b = df[df.group == 'intervention']['prod']    \n    \n    diff = a.mean() - b.mean()\n\n    res = ss.ttest_ind(a, b)\n      \n    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n    confint = means.tconfint_diff(alpha=alpha, alternative=alternative, usevar='unequal') \n    degfree = means.dof_satt()\n\n    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI', 'High95CI']\n    return pd.Series([degfree, \n                      diff, res[0], res[1], confint[0], confint[1]], index = index)   \n   \n\ntest = t_test_two_samp(dat, 0.05)\ntest",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The *t*-value of 3.16 tells us that the difference between our groups\n(top of *t*-test fraction) is 3.16 times larger than would be expected\ntypically by chance (bottom of *t*-test fraction). How often would a\nresult this big happen if the null were actually true? The *p*-value is\n.002, so only 0.2% of the time. This is sufficient to reject the null\n(*p* &lt; .05), and we can conclude that our difference is not due to\nchance. We also have a 95% CI on the size of the difference, and we are\nfairly confident that the control group is .457 to 0.107 productivity\npoints lower than intervention group.\n\nImportantly, because we performed a randomized, controlled experiment,\nwe can conclude that this was actually the result of our treatment. This\nis a good sign, but the size of the improvement is small. We can\nconclude that our intervention **did** improve productivity, but it was\nonly by about a quarter of a point.\n\nHow big is that? Well, the scale is a 1-7 scale. We can try plotting the data using a boxplot of the two groups to visualize it. The Seaborn code in the cell below creates a boxplot of the `prod` values grouped by the `group` variable. A `swarmplot` is superimposed so you can see the position of the data points. The swarm plot shows a jittered display of the data points.    "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ax = plt.figure(figsize=(8,8)).gca() # define axis\nsns.boxplot(x = 'group', y = 'prod', data = dat, ax = ax)\nsns.swarmplot(x = 'group', y = 'prod', color = 'black', data = dat, ax = ax, alpha = 0.4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](M3_Lab3_-_Causal_Claim_files/figure-markdown_strict/unnamed-chunk-5-1.png)\n\nA violin plot might also help to visualize the differences. The code in the cell below follows the same recipe used above, but with the `violinplot` function. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ax = plt.figure(figsize=(8,8)).gca() # define axis\nsns.violinplot(x = 'group', y = 'prod', data = dat, ax = ax)\nsns.swarmplot(x = 'group', y = 'prod', color = 'black', data = dat, ax = ax, alpha = 0.4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](M3_Lab3_-_Causal_Claim_files/figure-markdown_strict/unnamed-chunk-7-1.png)\n\nWe see from the above plots that, although the effect was statistically significant, the\ndifference is fairly minimal.\n\nWe can estimate our effect size (Cohen's *d*) using the `tt_ind_solve_power`\nfunction from the `statsmodels.stats.power` package:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#cohen.d(prod ~ group, data=dat)\ncontrol = dat[dat.group == 'control']['prod']\nintervention = dat[dat.group == 'intervention']['prod']\nprint(np.mean(intervention) - np.mean(control))\nratio = len(control)/len(intervention)\ntt_ind_solve_power(effect_size=None, nobs1 = len(control), alpha=0.05, power=0.8, ratio=ratio, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "From these results you can see that the actual difference between these groups is a bit larger than Cohen's d required from 0.80 power.\n\nConclusion\n==========\n\nThanks to this study, we can be fairly certain that the stress reduction\nintervention had an effect. However, the difference in the effect is\nminimal at best."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}