{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 3, Lab 2 - Association\n=============================\n\nIn this lab, we will examine how to analyze data for a correlation. Note\nthat a detailed dive into correlational and regression-based research is\ngiven in Module 5. However, a brief overview is provided here. I focus\non correlation because it is the simplest way to make an association\nclaim, but as we saw in the online lesson, actually the correct analysis\ndepends on your data (continuous, discrete, normal vs non-normal, etc.).\nThus, a full illustration of all association techniques would take many,\nmany labs. I focus on correlation here.\n\nIn this example, you are analyzing customer loyalty data. Your\norganization uses three measures of loyalty, and you wish to test them\nout. (To avoid discussions of popular real measures, we will name these\n`loytalty1`, `loyalty2`, and `loyalty3`).\n\nNote that this lab uses the `ggplot2` package for data visualization and\nthe `psych` package for correlation testing. I also assume you are\nfamiliar with `ggplot2`. As an alternative to the `psych` tools, we can\nalso use the `Hmisc` package for correlation testing."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#### LOAD PACKAGES ####\n## Use inline magic command so plots appear in the data frame\n%matplotlib inline\n\n## Next the packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as ss\nimport math",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You load the data from the CSV file in the github folder for this lab:"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#### LOAD DATA ####\ndat = pd.read_csv(\"datasets/loyaltydata.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You inspect the data:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(dat.columns)\n\ndat.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There is an ID variable shown as `Unnamed` and scores on a loyalty measures named\n`loyalty1` through `loyalty3`. \n\n****\n**Note:** You are not yet familiar with the scaling of these measures.\n***\n\nThe first thing to do is to explore the variables. The Pandas `describe()` method."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This gives you a sense as to the range and scaling of each loyalty\nmeasure.\n\nImagine that each loyalty measure was in common use. You might want to\nknow whether they are highly correlated. We can compute correlations\nbetween variables with the Pandas `corr()` method. A subset of the data frame is taken (using the outer `[]` operator) by providing a list (the inner `[]`) of column names."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat[['loyalty1', 'loyalty2', 'loyalty3']].corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This difficult to read. Let's use the Pandas `round()` method:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "corr_mat = dat[['loyalty1', 'loyalty2', 'loyalty3']].corr().round(2)\ncorr_mat",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that the variables are *not* highly correlated with each\nother. This is a potential problem.\n\nA brief refresher: correlations range between zero (no association\nbetween variables) and 1.0 (a one-to-one association). They can also be\npositive (as one variable increases, so does the other) or negative (as\none variable increases, the other decreases)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The statistician Jacob Cohen suggested the following guidelines:  \n\n<pre>\n| # | Correlation |  Meaning   |\n|---|-------------|------------|\n| 1 |  0.0 - 0.1  | Negligible |\n| 2 |  0.1 - 0.3  |   Small    |\n| 3 |  0.3 - 0.5  |   Medium   |\n| 4 |    0.5 +    |   Large    |\n</pre>\n\nHowever, given that they are all ostensibly measuring the same thing,\nloyalty, we should expect much higher correlations (.7-.9).\n\nWe can also easily visualize this correlation with using the `heatmap` function from the Python Seaborn package. Seaborn is a sophisticated statistical charting package. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.heatmap(corr_mat, vmax=1.0) \nplt.title('Correlation matrix for loyalty features')\nplt.yticks(rotation='horizontal')\nplt.xticks(rotation='vertical')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, you will create scatter plots of each pairwise combination of loyalty variables. The code uses the `lmplot` function from Seaborn. Jitter on both the x and y axes along with high point transparency are used to help deal with over-plotting. Notice that the transparency argument, `alpha`, must be passed to the underling Matplotlib in a dictionary called `scatter_kws`. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.lmplot(\"loyalty1\", \"loyalty2\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.lmplot(\"loyalty1\", \"loyalty3\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.lmplot(\"loyalty2\", \"loyalty3\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-8-1.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-8-3.png)\nAll of the graphs look about the same. It is always good to inspect the\nplots, as we know that non-linearity can weaken our correlations. Here,\nwe see evidence that each measure is correlated linearly; the\nassociations are simply underwhelming."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-9-1.png)\n\nWe can easily compute the confidence intervals of these correlation coefficients. However, this requires a few steps (don't worry if you don't follow this completely:   \n1. Transform the correlation from the initial space which we call r to a transformed space z. The distribution of errors is Normal in this transformed space. \n2. Compute the CI in the transformed space.\n3. Transform back to the original space."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def r_z(r):\n    return math.log((1 + r) / (1 - r)) / 2.0\n\ndef z_r(z):\n    e = math.exp(2 * z)\n    return((e - 1) / (e + 1))\n\ndef r_conf_int(r, alpha, n):\n    # Transform r to z space\n    z = r_z(r)\n    # Compute standard error and critcal value in z\n    se = 1.0 / math.sqrt(n - 3)\n    z_crit = ss.norm.ppf(1 - alpha/2)\n\n    ## Compute CIs with transform to r\n    lo = z_r(z - z_crit * se)\n    hi = z_r(z + z_crit * se)\n    return (lo, hi)\n\nprint('\\nFor loyalty1 vs. loyalty2')\ncorr_mat = np.array(corr_mat)\nconf_ints = r_conf_int(corr_mat[1,0], 0.05, 1000)\nprint('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[1,0], conf_ints[0], conf_ints[1]))\nprint('\\nFor loyalty1 vs. loyalty3')\nconf_ints = r_conf_int(corr_mat[2,0], 0.05, 1000)\nprint('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[2,0], conf_ints[0], conf_ints[1]))\nprint('\\nFor loyalty2 vs. loyalty3')\nconf_ints = r_conf_int(corr_mat[2,1], 0.05, 1000)\nprint('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[2,1], conf_ints[0], conf_ints[1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see that the CIs of all the correlation coefficients are relatively small compared to the correlation coefficients. This indicates that these coefficients are statistically significant."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What Claims Can We Make?\n========================\n\nHere, we can make the following claims: each of these variables is\ncorrelated with each other, but in reality, the correlations are weaker\nthan you would hope them to be. In this case, we can have a series of\nconversations about whether these measures of loyalty are assessing\ndifferent things, whether there are actually different kinds of customer\nloyalty, or whether the measures are not of high quality. Regardless,\nthere appears to *not* be a large association between our measures of\nloyalty. In fact, using 95% CIs, we found that we had fairly precise\nestimate of our correlations: they are not strong. This raises large\nimplications for our organization as it considers using these measures."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}