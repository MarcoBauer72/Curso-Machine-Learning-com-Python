{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 3, Lab 1 - Frequency Claims\n==================================\n\nIn this lab, we will examine how to analyze data for a frequency claim.\nA frequency claim is where the level of a single variable is reported.\n\nIn this example, you are analyzing data from a local coffee company. You\nwish to know how many coffee beverages are consumed by the average\ncustomer in a day. These customers are surveyed and the data are\nproduced. You load the data from a CSV file (in the datasets github folder for\nthis lab).\n\nThe packages are loaded as a first step. You can safely ignore any deprecation warning which may appear depending on the version of the `statsmodels` package being loaded. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#### LOAD PACKAGES ####\n## Use inline magic command so plots appear in the data frame\n%matplotlib inline\n\n## Next the packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew\nimport statsmodels.stats.api as sms",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we load our data as a Pandas data frame:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#### LOAD DATA ####\ndat = pd.read_csv(\"datasets/cupsdat.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You inspect the data using the Pandas `columns` attribute and `head()` function:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(dat.columns)\n\ndat.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There is an ID variable called `Unnamed` and a variable indicating the number of\nbeverages named `count`.\n\nThe first thing to do is to explore the variable. The Pandas `describe()`\nmethod has many useful features."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat['count'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here we see that scores range from 0-7, with a median (50% quantile) of 2.\n\nExploring Counts\n================\n\nWe can also use the `value_counts()` method on a Pandas series (single column of vector):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat['count'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This gives us a sense of the distribution. The values are in the left column and the counts in the right column. \n\nOften, stakeholders want percentages. This is easy to accomplish, provided you know how many responses you have. The number of rows in the data frame can be returned as the first value of the shape attribute:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Or, you could ask for the `len()` of the `dat$count` variable:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(dat['count'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "However, I would avoid these as there could be missing values in the data frame. Treating missing values in Pandas data frames is a bit complicated as there can be multiple ways missing values are coded. The simple general recipe for finding and treating missing values is:\n1. Determine how missing values are coded. They might be coded as `numpy.nan` (not a number), `numpy.inf` (infinity) or some other code.\n2. Convert all missing values to `numpy.nan` values.\n3. Use the Pandas `dropna()` method.   \n\nFollowing this recipe, the percentages of each value can be given by dividing each count by\nthe total is computed as shown."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Remove rows with nan without making copy of the data frame\ndat.dropna(axis = 0, inplace = True) \n\n## Now get the counts into a data frame sorted by the number\ncount_frame = dat['count'].value_counts()\ncount_frame = pd.DataFrame({'number':count_frame.index, 'counts':count_frame}).sort_values(by = 'number')\n\n## Compute the percents for each number\nn = len(dat['count'])\ncount_frame['percents'] = [100* x/n for x in count_frame['counts']]\n\n## Print as a nice table\ncount_frame[['number', 'percents']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, it can sometimes be helpful to generate a cumulative\npercentage. This can be done with the `cumsum()` method:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Add a cumsum dat\ncount_frame['cumsums'] = count_frame['percents'].cumsum()\n## Print as a nice table\ncount_frame[['number', 'percents', 'cumsums']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here easily that 60% of the sample has consumed 2 drinks per day\nor fewer. This is a very handy little chart.\n\nHistogram\n=========\n\nThe most common data visualization is a histogram:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.hist(dat['count'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](M3_Lab1_-_Frequency_files/figure-markdown_strict/unnamed-chunk-12-1.png)\n\nWe see here that the most common score is 1 and that that data has considerable skew. But, notice there are gaps in the histogram bars. You know from the frequency table that there should be no gaps. \n\nThe problem with the above histogram is that the default number of bins was used. Using 8 bins (for the 8 possible count values) will give a more representative histogram. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.hist(dat['count'], bins = 8)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](M3_Lab1_-_Frequency_files/figure-markdown_strict/unnamed-chunk-13-1.png)\n\nThis looks both professional and more accurate. Changing plot properties can change your perception of data.\n\nTo make a better graph for a presentation you can add plot attributes, such as axis labels and a title:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.hist(dat['count'], bins = 8)\nplt.title('Frequency of number of cups of coffee consumed')\nplt.xlabel('Cups of coffee per day')\nplt.ylabel('Frequency')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\nCentral Tendency\n================\n\nAssuming you want to provide a one-number summary, you can provide an\naverage. However, we see here given the skew that the mean will be\nbiased upwards.\n\nUsing the `skew()` function from the `scipy.stats` package, we can see this is\na modestly skewed distribution:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "skew(dat['count'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is within acceptable range for many purposes (any analyses start to\nworry when skew reaches somewhere between 0.80-2.0). You can compute the mean and median of an array using the Numpy `mean()` and `median()` functions:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(np.mean(dat['count']))\nprint(np.median(dat['count']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Before you finish, you might want to put a confidence interval around\nthe mean. You can use the `CI()` command from the `Rmisc` package,\nwhich works well for analysis when you plan to analyze the mean:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sms.DescrStatsW(list(dat['count'])).tconfint_mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you wished to provide a CI for a median, or if your data are\nproportions or some other format than these, there are many easy options\nthat can be found with a brief web search, similar to the above.\n\nConclusion\n==========\n\nIn this case, we can make a frequency claim: most people, on average\nconsume 1-2 cups of coffee per day."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}