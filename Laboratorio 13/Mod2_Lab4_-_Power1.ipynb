{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 2, Lab 4 - Power 1\n=========================\n\nIn this lab, we explore the concept of statistical power. Recall that\npower is the ability to reject the null hypothesis. Greater power means\ngreater ability to reject the null hypothesis. Of course, if you cannot\nreject the null hypothesis, you cannot rule out random chance as an\nexplanation for your findings. Therefore, the ability to reject the null\nhypothesis should be considered a bare minimum for any research or data\nscience finding that relies on samples to estimate populations. In fact,\nas we saw in the confidence interval lab, it's often desirable to be\neven more precise than rejecting the null hypothesis (i.e., more precise\nthan \"effect is not zero\"). Still, good power is a bare minimum\nrequirement.\n\nNote: you will need the `effsize` package for this lab to measure effect\nsizes; you will also need the `pwr` package. We will do some data\nvisualization with `ggplot2` as well:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "### LOAD PACKAGES ####\nfrom scipy import stats",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that for this lab you need to be familiar with the effect size\nstatistic, Cohen's *d*, discussed in the online lesson. I will assume\nyou have watched this and are familiar with how *p*-values work to\nreject the null hypothesis. If you need to refresh these concepts, do so\nbefore completing this lab. The standard guidelines for Cohen's *d* are:\n<pre>\n| #  |    d Value    |  Meaning   |\n|:--:|:-------------:|:----------:|\n| 1. |    0 - 0.2    | Negligible |\n| 2. |   0.2 - 0.5   |   Small    |\n| 3. |   0.5 - 0.8   |   Medium   |\n| 4. |     0.80 +    |   Large    |\n</pre>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Power Overview\n==============\n\nTechnically defined, power is the proportion of the time in which you\nwould achieve *p* &lt; .05 for a given population effect size. Power is\nrelevant *only* when the null hypothesis is false. Assuming that the\nnull is false, what is the likelihood that your study will actually\ndetect the effect that is truly present? For example, a study that has\nonly 20% power has only a 20% chance to actually detect the effect that\nis present (i.e., achieve statistical significance, *p* &lt; .05).\nClearly, you do not want your study to be set up to fail. You want good\npower...usually the recommendation is .80 or higher.\n\nPower is **highly dependnet on sample size**.\n\nFor example, imagine that, in the population, the size of the difference\nbetween two groups (e.g., \"intervention group to increase productivity\"\nvs. \"business as usual\" group) was *d* = .4. A study with *n*= 40 people\nper group would only have 42% power, i.e., be able to reject the null\nhypothesis 42% of the time.\n\nHow do I know that? I used a power calculator.\n\nThis can be easily calculated using the `tt_ind_solve_power` function in the\n`statsmodels.stats.power` Python package. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from statsmodels.stats.power import tt_ind_solve_power\ntt_ind_solve_power(effect_size=0.4, nobs1 = 40, alpha=0.05, power=None, ratio=1, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that the power is given as `power = 0.4235212`.\n\nPower is a function of both the effect size and the sample size. Imagine\nthat we had a larger sample, say *n* = 100 people per group. Would our\npower improve?"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tt_ind_solve_power(effect_size=0.4, nobs1 = 100, alpha=0.05, power=None, ratio=1, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Indeed, now we have reached 80% power, the standard guideline for\nacceptable power.\n\nWe could see what happens if we use larger groups, say 150 per group:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tt_ind_solve_power(effect_size=0.4, nobs1 = 150, alpha=0.05, power=None, ratio=1, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we have 93% power. You always want high power. If power is low, you\nrisk missing real effects, spending time and energy on a study coming up\nempty even when there is something to be found.\n\nTo reiterate, in power calculations, there are always three things that\ninterrelate:\n\n1.  Power\n2.  Sample size (*n* per group in group-comparison studies)\n3.  Effect size\n\nIn most power calculators, you enter any two of those three, and the\nthird can be determined for you.\n\nFor example, say we are planning a study and want to know how many\nresponses are required. We can specify the desired power (leave `n`\nblank) and the command will find the required sample size:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tt_ind_solve_power(effect_size=0.4, nobs1 = None, alpha=0.05, power=0.8, ratio=1, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that this study would require 100 people per group (best to\nround up).\n\nHowever, the relationship between sample size and power is not fixed. In\nfact, power also goes down as effect size goes down. This should make\nsense: it is much harder to detect a very tiny effect than it is a very\nlarge effect. Let's re-run the last command, asking how many people are\nneeded for 80% power but now considering the situation where the\npopulation *d* is smaller, *d* = .20:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tt_ind_solve_power(effect_size=0.2, nobs1 = None, alpha=0.05, power=0.8, ratio=1, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see now that our required sample size has jumped to 394 per group, a\nmuch larger and more expensive study.\n\nSo, as you plan studies, you need to tell a power calculator the desired\npower level (recommended: 80% or .80) *and* the population effect size.\n\n**However, you don't know the population effect size. If you did, you\nwouldn't be running a research study, would you?**\n\nStandard practice, therefore, is to input the smallest effect size you\nwould care to be able to detect (called \"smallest effect size of\ninterest, or SESOI).\n\nFor example, perhaps your research question is *very important* and you\nwant to be able to detect any effect, even if it is small. In that case,\npower the study as above with 394 people per group.\n\nOr, perhaps a small effect is unimportant to you; you care to know if an\neffect is present but really only if it's large (you're fine getting a\nnull result if the effect is small; small effects are not important to\nyou). In that case, you can use a more lenient criteria:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tt_ind_solve_power(effect_size=0.8, nobs1 = None, alpha=0.05, power=0.8, ratio=1, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This example would let us have 80% power to detect a large effect with\nonly 26 people per group. However, the high power is only present when\nthe effect is that large. if the effect were smaller, your power would\nbe worse and you would likely miss it. Let's take that same sample size\nand consider what would happen if your effects were smaller than *d* =\n.80. Try running the command for *nobs* = 26 and *d* = .30. What is the\npower?"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tt_ind_solve_power(effect_size=0.3, nobs1 = 26, alpha=0.05, power=None, ratio=1, alternative='two-sided')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Yikes! For that 26-person-per-group study, if the population effect size\nwere *d* = .30, we would have only an 18% chance of detecting the\neffect. Thus, the *same* study can have very good power to detect large\neffects but terrible power to detect small effects. Thus, the researcher\nshould always decide the smallest effect size of interest and power the\nstudy to detect that.\n\nJust to play with this some more, let's illustrate how a sample size of\n26 per group performs across a variety of effect sizes. I will use a\nloop command to do this."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nd_vals = [x/10.0 for x in range(1, 16)]\npowers = [tt_ind_solve_power(effect_size=x, nobs1 = 26, alpha=0.05, power=None, ratio=1, alternative='two-sided') \n                    for x in d_vals]\nd_powers = pd.DataFrame({'d_values':d_vals, 'power':powers})\nd_powers",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Using that sample size of 26, we see that we have excellent power to\ndetect large effect sizes (Cohen *d* values .80 and above), but we would\nhave very low power (very likely to get a non-significant result) if the\neffect is small. In such cases, we would likely come back and say \"we\ndid not find a difference\" when really we simply missed what was there.\n\nThus, you must always decide the smallest effect size you want to be\nable to have good power to detect. This should be based on the\nimportance of the question, resources available, and how big you guess\nthe effect might be. If you are chasing very nuanced things in noisy\ndata, you may need very large samples to get statistical significance.\n\nPlanning Sample Size\n====================\n\nSometimes it can be handy to generate a number of power estimates for\ndifferent effect sizes and sample sizes. The code in the cell below performs the following processing:\n1. A list of d values is created with a list comprehension.\n2. A data frame is created with a single column for sample sizes.\n3. Loop over the d values. Within the loop a list comprehension is used to compute the power of the test for each sample size. \n\nExecute this code:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "## create list of d values\nd_vals = [x/10.0 for x in range(2,16)]\n\n## Initialize data frame\npowers = pd.DataFrame({'sample_size':range(20,210,10)})\n## Loop over d values\nfor d_val in d_vals:\n    col_name = 'd = ' + str(d_val)\n    ## List comprehension for each d value itterating over the sample sizes\n    powers[col_name] = [tt_ind_solve_power(effect_size=d_val, nobs1 = x, alpha=0.05, power=None, ratio=1, alternative='two-sided')\n                      for x in range(20,210,10)]\n        \npowers   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Execute the code in the cell below to plot power vs. the sample size for each of the d values:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline\nfig = plt.figure(figsize=(12,10)) # define plot area\nax = fig.gca() # define axis\npowers.plot(x = 'sample_size', ax = ax, linestyle = '-.')\nplt.hlines(y = 0.8, xmin = 20, xmax = 200, color = 'red', linestyle = '--')\nplt.title('Power vs. sample size for values of d')\nplt.ylabel('Power')\nplt.xlabel('Sample Size')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can modify the above code for different sample sizes and ranges, but\nthe rest stays the same. You can easily see with the graph that 80%\npower (red dashed line) would take 180 participants per group for a *d*\n= .03 but would only take 45 people per group at *d* = .06."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One Final Word of Caution\n=========================\n\nSome people have brushed aside the issue of poor power, especially when\na result is significant. After all, if a result is significant, it *had*\npower to reject the null, right?\n\nWrong. Beware significant findings from under-powered studies.\n\nConsider a study that has very low power. In such a case, the researcher\nhas almost *no* ability to find true effects.\n\nHowever, false positives *still happen* 5% of the time (when the null is\ntrue).\n\nThus, as power gets lower, the number of \"true\" positives (real effects,\ndiscovered) goes down, while the number of false positives stays fixed.\nWhen power is low, false positives can equal or even outnumber true\npositives. In such cases, when an effect is significant, it is very\nchallenging to know whether the effect is a true positive or a false\npositive. We will explore this issue in a future lesson, but take this\nas a word of caution about under-powered studies."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}